# -*- coding: utf-8 -*-
"""201021_02_DeepLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11WxxR-uPwzWn7IXpS9bnCW0izVcbl28c
"""

import tensorflow as tf
import keras
import numpy as np
import sys
import pandas as pd

### 머신러닝 - 딥러닝

from keras.models import Sequential
from keras.layers import Dense

# 십진 숫자를 0,1의 구성으로

from keras.utils import np_utils 
from keras.datasets import mnist

"""### 첫번째 모델 - 기본 모델"""

(x_train, y_train),(x_test,y_test)=mnist.load_data()

X_train = x_train.reshape(60000, 784).astype('float32') / 255.0
X_test = x_test.reshape(10000, 784).astype('float32') / 255.0
Y_train = np_utils.to_categorical(y_train)
Y_test = np_utils.to_categorical(y_test)

model = Sequential()
model.add( Dense(units=16, input_dim=28*28, activation='tanh'))  # 입력층(28*28=784노드) - 은닉층(64개노드)
model.add(Dense(units=10, activation='softmax'))                # 출력층(10개 노드)

model.compile(loss='categorical_crossentropy', 
              optimizer='sgd', metrics=['accuracy'])

# hist = model.fit(x_train, y_train, epochs=5, batch_size=32)
hist = model.fit(X_train, Y_train, epochs=10, batch_size=32)

loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)
print('## evaluation loss and_metrics ##')
print(loss_and_metrics)  # 최종 데이터 loss와 정확도(accuracy)

"""### 두번째 모델 - 은닉층 2개"""

model = Sequential()
model.add( Dense(units=16, input_dim=28*28, activation='tanh'))  # 입력층(28*28=784노드) - 은닉층(64개노드)
model.add( Dense(16, activation='tanh'))
model.add( Dense(16, activation='tanh'))
model.add(Dense(units=10, activation='softmax'))                # 출력층(10개 노드)

model.compile(loss='categorical_crossentropy', 
              optimizer='sgd', metrics=['accuracy'])
hist = model.fit(X_train, Y_train, epochs=10, batch_size=32)

loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)
print('## evaluation loss and_metrics ##')
print(loss_and_metrics)  # 최종 데이터 loss와 정확도(accuracy)

"""### 세번째 모델 - 활성화 함수 tanh -> relu
* tanh 에서 relu로 가면 오차범위 내의 동일성능을 보인다.
"""

model = Sequential()
model.add( Dense(units=16, input_dim=28*28, activation='relu'))  # 입력층(28*28=784노드) - 은닉층(64개노드)
model.add( Dense(16, activation='tanh'))
model.add( Dense(16, activation='tanh'))
model.add(Dense(units=10, activation='softmax'))                # 출력층(10개 노드)

model.compile(loss='categorical_crossentropy', 
              optimizer='sgd', metrics=['accuracy'])
hist = model.fit(X_train, Y_train, epochs=10, batch_size=32)

loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)
print('## evaluation loss and_metrics ##')
print(loss_and_metrics)  # 최종 데이터 loss와 정확도(accuracy)

"""### 네번째 모델 - 노드 수를 64개로 늘려보기
* 은닉층의 노드수를 늘렸더니 성능의 향상이 있었음.
"""

model = Sequential()
model.add( Dense(units=64, input_dim=28*28, activation='relu'))
model.add( Dense(16, activation='tanh'))
model.add( Dense(16, activation='tanh'))
model.add(Dense(units=10, activation='softmax'))

model.compile(loss='categorical_crossentropy', 
              optimizer='sgd', metrics=['accuracy'])
hist = model.fit(X_train, Y_train, epochs=10, batch_size=32)

loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)
print('## evaluation loss and_metrics ##')
print(loss_and_metrics)

"""### 4-1 노드 수를 미친듯이 늘려보자"""

model = Sequential()
model.add( Dense(units=8196, input_dim=28*28, activation='relu'))
model.add( Dense(16, activation='tanh'))
model.add( Dense(16, activation='tanh'))
model.add(Dense(units=10, activation='softmax'))

model.compile(loss='categorical_crossentropy', 
              optimizer='sgd', metrics=['accuracy'])
hist = model.fit(X_train, Y_train, epochs=10, batch_size=32)

loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)
print('## evaluation loss and_metrics ##')
print(loss_and_metrics)

